{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification decision tree model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "# Decision Tree Classifier (with Gini impurity)\n",
    "class DecisionTree:\n",
    "    # Initialize the Decision Tree Classifier\n",
    "    def __init__(self, min_samples_split=2, max_depth=5):\n",
    "        # Minimum number of samples required to split an internal node\n",
    "        self.min_samples_split = min_samples_split\n",
    "        # Maximum depth of the tree\n",
    "        self.max_depth = max_depth\n",
    "        # The decision tree\n",
    "        self.tree = None\n",
    "\n",
    "    # Fit the Decision Tree Classifier\n",
    "    def fit(self, X, y):\n",
    "        # Convert string labels to numerical indices\n",
    "        self.classes = np.unique(y)\n",
    "        y = np.array([np.where(self.classes == label)[0][0] for label in y])\n",
    "        # Grow the decision tree\n",
    "        self.tree = self._grow_tree(X, y)\n",
    "    \n",
    "    # Predict the target variable for the input data\n",
    "    def predict(self, X):\n",
    "        predictions = [self._predict(inputs, self.tree) for inputs in X]\n",
    "        # Make predictions for each input based on the decision tree that was grown\n",
    "        return np.array([self.classes[pred] for pred in predictions])\n",
    "\n",
    "    # Calculate the Gini impurity\n",
    "    def _gini(self, y):\n",
    "        # Number of samples\n",
    "        m = len(y)\n",
    "        # Return 1 - sum of the square of the proportion of samples in each class label to the total number of samples in the node\n",
    "        return 1.0 - sum((np.sum(y == c) / m) ** 2 for c in np.unique(y))\n",
    "\n",
    "    # Split the data into left and right subsets based on the feature and threshold\n",
    "    def _split(self, X, y, idx, thresh):\n",
    "        # Get the indices of the samples in the left subset\n",
    "        left_mask = X[:, idx] <= thresh\n",
    "        # Get the indices of the samples in the right subset\n",
    "        right_mask = X[:, idx] > thresh\n",
    "        # Return the left and right subsets of the data\n",
    "        return X[left_mask], X[right_mask], y[left_mask], y[right_mask]\n",
    "\n",
    "\n",
    "    # Find the best split for the data\n",
    "    def _best_split(self, X, y):\n",
    "        # Number of samples (m) and number of features (n)\n",
    "        m, n = X.shape\n",
    "\n",
    "        # If the number of samples is less than or equal to the minimum number of samples required to split, return None\n",
    "        if m <= 1:\n",
    "            return None, None\n",
    "\n",
    "        # Initialize the Gini impurity of the best split to infinity and the best feature and threshold to None\n",
    "        best_gini = 1.0\n",
    "        best_idx, best_thresh = None, None\n",
    "\n",
    "        unique_classes = np.unique(y)\n",
    "        class_count = len(unique_classes)\n",
    "\n",
    "        # For each feature\n",
    "        for idx in range(n):\n",
    "            # Get the thresholds and class labels\n",
    "            thresholds, classes = zip(*sorted(zip(X[:, idx], y)))\n",
    "\n",
    "            # Ensure numeric thresholds\n",
    "            thresholds = np.array(thresholds, dtype=np.float64)\n",
    "\n",
    "            num_left = [0] * class_count\n",
    "            num_right = [np.sum(classes == c) for c in unique_classes]\n",
    "\n",
    "            # For each sample\n",
    "            for i in range(1, m):\n",
    "                class_idx = np.where(unique_classes == classes[i - 1])[0][0]\n",
    "                num_left[class_idx] += 1\n",
    "                num_right[class_idx] -= 1\n",
    "\n",
    "                gini_left = 1.0 - sum((num_left[x] / i) ** 2 for x in range(class_count))\n",
    "                gini_right = 1.0 - sum((num_right[x] / (m - i)) ** 2 for x in range(class_count))\n",
    "                gini = (i * gini_left + (m - i) * gini_right) / m\n",
    "\n",
    "                if thresholds[i] == thresholds[i - 1]:\n",
    "                    continue\n",
    "                if gini < best_gini:\n",
    "                    best_gini = gini\n",
    "                    best_idx = idx\n",
    "                    best_thresh = (thresholds[i] + thresholds[i - 1]) / 2\n",
    "        \n",
    "        return best_idx, best_thresh\n",
    "\n",
    "    # Grow the decision tree\n",
    "    def _grow_tree(self, X, y, depth=0):\n",
    "        \n",
    "        # Get the number of samples for each class label\n",
    "        num_samples_per_class = [np.sum(y == i) for i in np.unique(y)]\n",
    "        # Get the class that occurs\n",
    "        predicted_class = np.argmax(num_samples_per_class)\n",
    "        # Create a node for the decision tree\n",
    "        node = {'predicted_class': predicted_class}\n",
    "\n",
    "        \n",
    "\n",
    "        # If the depth of the tree is less than the maximum depth\n",
    "        if depth < self.max_depth:\n",
    "            # Get the best split\n",
    "            idx, thresh = self._best_split(X, y)\n",
    "            # If the best split is not None\n",
    "            if idx is not None:\n",
    "                # Get the left and right subsets of the data\n",
    "                indices_left = X[:, idx] <= thresh\n",
    "                X_left, y_left = X[indices_left], y[indices_left]\n",
    "                X_right, y_right = X[~indices_left], y[~indices_left]\n",
    "                # Grow the left and right subtrees\n",
    "                node['feature_index'] = idx\n",
    "                node['threshold'] = thresh\n",
    "                node['left'] = self._grow_tree(X_left, y_left, depth + 1)\n",
    "                node['right'] = self._grow_tree(X_right, y_right, depth + 1)\n",
    "        return node\n",
    "\n",
    "    # Predict the target variable for the input data\n",
    "    def _predict(self, inputs, tree):\n",
    "        # If the tree is not a leaf node\n",
    "        if 'threshold' in tree:\n",
    "            # Get the feature index and threshold\n",
    "            feature_index = tree['feature_index']\n",
    "            # Traverse the left or right subtree\n",
    "            if inputs[feature_index] <= tree['threshold']:\n",
    "                return self._predict(inputs, tree['left'])\n",
    "            else:\n",
    "                return self._predict(inputs, tree['right'])\n",
    "        # If the tree is a leaf node\n",
    "        else:\n",
    "            # Return the predicted class\n",
    "            return tree['predicted_class']\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressor decision tree model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# from Models.ClassifierDT import DecisionTree\n",
    "\n",
    "# Decision Tree Regressor (with SSR)\n",
    "class DecisionTreeRegressor(DecisionTree):\n",
    "    # Initialize the Decision Tree Regressor that inherits from the Decision Tree Classifier\n",
    "    def _ssr(self, y):\n",
    "        if len(y) == 0:\n",
    "            return 0\n",
    "        mean_y = np.mean(y)\n",
    "        # Calculate the sum of the squared residuals and return it\n",
    "        return np.sum((y - mean_y) ** 2)\n",
    "\n",
    "    # Find the best split for the data based on the sum of the squared residuals\n",
    "    def _best_split(self, X, y):\n",
    "\n",
    "        # Number of samples (m) and number of features (n)\n",
    "        m, n = X.shape\n",
    "\n",
    "        # If the number of samples is less than or equal to the minimum number of samples required to split, return None\n",
    "        if m <= 1:\n",
    "            return None, None\n",
    "\n",
    "        # Initialize the sum of the squared residuals of the best split to infinity and the best feature and threshold to None\n",
    "        best_ssr = np.inf\n",
    "        best_idx, best_thresh = None, None\n",
    "\n",
    "        # For each feature\n",
    "        for idx in range(n):\n",
    "            # Get the thresholds and values\n",
    "            thresholds, values = zip(*sorted(zip(X[:, idx], y)))\n",
    "            # For each sample\n",
    "            for i in range(1, m):\n",
    "                # Get the left and right subsets of the target variable\n",
    "                y_left, y_right = values[:i], values[i:]\n",
    "                ssr_left, ssr_right = self._ssr(y_left), self._ssr(y_right)\n",
    "\n",
    "                # Calculate the sum of the squared residuals of the left and right subsets\n",
    "                ssr = ssr_left + ssr_right\n",
    "\n",
    "                # Update the best split if the current split has a lower sum of the squared residuals\n",
    "                if thresholds[i] == thresholds[i - 1]:\n",
    "                    continue\n",
    "                if ssr < best_ssr:\n",
    "                    best_ssr = ssr\n",
    "                    best_idx = idx\n",
    "                    best_thresh = (thresholds[i] + thresholds[i - 1]) / 2\n",
    "\n",
    "        # Return the best feature and threshold\n",
    "        return best_idx, best_thresh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# from Models.ClassifierDT import DecisionTree\n",
    "\n",
    "\n",
    "class RandomForest:\n",
    "    def __init__(self, n_trees=100, max_depth=5, min_samples_split=2, n_features=None):\n",
    "        self.n_trees = n_trees\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.n_features = n_features\n",
    "        self.trees = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.trees = []\n",
    "        n_samples, n_features = X.shape\n",
    "        self.n_features = self.n_features or n_features\n",
    "        for _ in range(self.n_trees):\n",
    "            idxs = np.random.choice(n_samples, n_samples, replace=True)\n",
    "            tree = DecisionTree(max_depth=self.max_depth, min_samples_split=self.min_samples_split)\n",
    "            tree.fit(X[idxs], y[idxs])\n",
    "            self.trees.append(tree)\n",
    "\n",
    "    def predict(self, X):\n",
    "        tree_preds = np.array([tree.predict(X) for tree in self.trees])\n",
    "        return np.mean(tree_preds, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest imputation for missing values in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def random_forest_impute(data, categorical_columns, n_trees=100, max_depth=5, min_samples_split=2, n_features=None, max_iter=10, tol=1e-3):\n",
    "    data_imputed = data.copy()\n",
    "    missing_mask = data.isnull()\n",
    "    n_features = n_features or data.shape[1]\n",
    "    \n",
    "    # One-hot encode categorical variables\n",
    "    data_imputed = pd.get_dummies(data_imputed, columns=categorical_columns, dummy_na=True)\n",
    "    \n",
    "    for iteration in tqdm(range(max_iter), desc=\"Imputing missing values\"):\n",
    "        prev_data = data_imputed.copy()\n",
    "        for column in categorical_columns:\n",
    "            cat_columns = data_imputed.filter(like=column).columns\n",
    "            for cat_column in cat_columns:\n",
    "                missing_idx = data_imputed[cat_column].isnull()\n",
    "                if missing_idx.any():\n",
    "                    # Prepare training data\n",
    "                    X_train = data_imputed[~missing_idx].drop(columns=cat_columns)\n",
    "                    y_train = data_imputed[~missing_idx][cat_column]\n",
    "                    \n",
    "                    # Prepare data for imputation\n",
    "                    X_impute = data_imputed[missing_idx].drop(columns=cat_columns)\n",
    "                    \n",
    "                    # Train Random Forest\n",
    "                    rf = RandomForest(n_trees=n_trees, max_depth=max_depth, min_samples_split=min_samples_split, n_features=n_features)\n",
    "                    rf.fit(X_train.values, y_train.values)\n",
    "                    \n",
    "                    # Predict missing values\n",
    "                    data_imputed.loc[missing_idx, cat_column] = rf.predict(X_impute.values)\n",
    "        \n",
    "        # Check for convergence\n",
    "        if np.linalg.norm(data_imputed.values - prev_data.values) < tol:\n",
    "            break\n",
    "\n",
    "    return data_imputed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest regressor model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# from Models.ClassifierDT import DecisionTree\n",
    "# from Models.RandomForest import RandomForest\n",
    "\n",
    "# Random Forest Regressor (with averaging)\n",
    "class RandomForestRegressor(RandomForest):\n",
    "    def predict(self, X):\n",
    "        tree_preds = np.array([tree.predict(X) for tree in self.trees])\n",
    "        return np.mean(tree_preds, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries and modules\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# from Models.ClassifierDT import DecisionTree\n",
    "# from Models.RegressionDT import DecisionTreeRegressor\n",
    "# from Models.RandomForest import RandomForest\n",
    "# from Models.RandomForestReg import RandomForestRegressor\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "######### Load the dataset\n",
    " \n",
    "# Load the dataset\n",
    "data = pd.read_csv('/Users/edeneldar/Library/Mobile Documents/com~apple~CloudDocs/ML learn.worktrees/origin/mainEden/assignment1/assignment-1-data.csv')\n",
    "\n",
    "data = data[['Brand', 'Screen_Size', 'RAM', 'Processor', 'GPU', 'GPU_Type', 'Resolution', 'Condition', 'Price']]\n",
    "\n",
    "\n",
    "# # handle midding values\n",
    "# processors_catagory = data['Processor'].unique()\n",
    "# len(processors_catagory)\n",
    "\n",
    "# #  create histrograms of the data to see how each catagory is distributed for the GPU, GPU_Type, and Resolution columns under each processor type\n",
    "# for processor in processors_catagory:\n",
    "#     fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
    "#     for i, column in enumerate(['GPU', 'GPU_Type', 'Resolution']):\n",
    "#         sns.histplot(data[data['Processor'] == processor][column], ax=ax[i])\n",
    "#         ax[i].set_title(f'{column} for {processor}')\n",
    "#     plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing values imputation with random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['GPU', 'GPU_Type', 'Resolution'], dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m categorical_columns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGPU\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGPU_Type\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mResolution\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Impute missing values using Random Forest\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m imputed_data \u001b[38;5;241m=\u001b[39m random_forest_impute(data, categorical_columns, n_trees\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, min_samples_split\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, n_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, tol\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m)\n",
      "Cell \u001b[0;32mIn[14], line 7\u001b[0m, in \u001b[0;36mrandom_forest_impute\u001b[0;34m(data, categorical_columns, n_trees, max_depth, min_samples_split, n_features, max_iter, tol)\u001b[0m\n\u001b[1;32m      4\u001b[0m n_features \u001b[38;5;241m=\u001b[39m n_features \u001b[38;5;129;01mor\u001b[39;00m data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# One-hot encode categorical variables\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m data_imputed \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mget_dummies(data_imputed, columns\u001b[38;5;241m=\u001b[39mcategorical_columns, dummy_na\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m iteration \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(max_iter), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImputing missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     10\u001b[0m     prev_data \u001b[38;5;241m=\u001b[39m data_imputed\u001b[38;5;241m.\u001b[39mcopy()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/reshape/encoding.py:164\u001b[0m, in \u001b[0;36mget_dummies\u001b[0;34m(data, prefix, prefix_sep, dummy_na, columns, sparse, drop_first, dtype)\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput must be a list-like for parameter `columns`\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m     data_to_encode \u001b[38;5;241m=\u001b[39m data[columns]\n\u001b[1;32m    166\u001b[0m \u001b[38;5;66;03m# validate prefixes and separator to avoid silently dropping cols\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_len\u001b[39m(item, name: \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py:3899\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3897\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   3898\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 3899\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39m_get_indexer_strict(key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   3901\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   3902\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:6115\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6112\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6113\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6115\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_if_missing(keyarr, indexer, axis_name)\n\u001b[1;32m   6117\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   6118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6119\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:6176\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6174\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_interval_msg:\n\u001b[1;32m   6175\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 6176\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6178\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m   6179\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of [Index(['GPU', 'GPU_Type', 'Resolution'], dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "# Specify the categorical columns\n",
    "categorical_columns = ['GPU', 'GPU_Type', 'Resolution']\n",
    "\n",
    "# Impute missing values using Random Forest\n",
    "imputed_data = random_forest_impute(data, categorical_columns, n_trees=100, max_depth=5, min_samples_split=2, n_features=None, max_iter=10, tol=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Correct the values of the 'Condition' column to 'New' and 'Refurbished'\n",
    "data['Condition'] = data['Condition'].apply(lambda x: 'New' if x == 'New' or x == 'Open box' else 'Refurbished')\n",
    "\n",
    "# Convert categorical features to numerical values using one-hot encoding\n",
    "data = pd.get_dummies(data, columns=['Brand', 'Processor', 'GPU', 'GPU_Type', 'Resolution'])\n",
    "\n",
    "reg_data = pd.get_dummies(data, columns=['Condition'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the data into training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### Split the data\n",
    "\n",
    "# Split the data\n",
    "train_data = data.iloc[0:2058]\n",
    "val_data = data.iloc[2058:2499]\n",
    "test_data = data.iloc[2499:2939]\n",
    "\n",
    "# Split the data for regression\n",
    "train_data_reg = reg_data.iloc[0:2058]\n",
    "val_data_reg = reg_data.iloc[2058:2499]\n",
    "test_data_reg = reg_data.iloc[2499:2939]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract features and target variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features and target variables\n",
    "X_train_clas = train_data.drop(columns=['Condition'])\n",
    "X_train_reg = train_data_reg.drop(columns=['Price'])\n",
    "y_train_clas = train_data['Condition']\n",
    "y_train_reg = train_data_reg['Price']\n",
    "\n",
    "X_val_clas = val_data.drop(columns=['Condition'])\n",
    "X_val_reg = val_data_reg.drop(columns=['Price'])\n",
    "y_val_clas = val_data['Condition']\n",
    "y_val_reg = val_data_reg['Price']\n",
    "\n",
    "X_test_clas = test_data.drop(columns=['Condition'])\n",
    "X_test_reg = test_data_reg.drop(columns=['Price'])\n",
    "y_test_clas = test_data['Condition']\n",
    "y_test_reg = test_data_reg['Price']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert string labels to numerical indices for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert string labels to numerical indices\n",
    "class_map = {label: idx for idx, label in enumerate(np.unique(y_train_clas))}\n",
    "y_train_clas_numeric = np.array([class_map[label] for label in y_train_clas])\n",
    "y_val_clas_numeric = np.array([class_map[label] for label in y_val_clas])\n",
    "y_test_clas_numeric = np.array([class_map[label] for label in y_test_clas])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision tree training and prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7959183673469388\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate Decision Tree Classifier with max_depth=5\n",
    "dt_classifier = DecisionTree(max_depth=5)\n",
    "dt_classifier.fit(X_train_clas.values, y_train_clas_numeric)\n",
    "predictions = dt_classifier.predict(X_val_clas.values)\n",
    "accuracy = np.mean(predictions == y_val_clas_numeric)\n",
    "print(f'Validation Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision tree regressor training and prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation MSE: 177610.21589024944\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate Decision Tree Regressor with max_depth=5\n",
    "dt_regressor = DecisionTreeRegressor(max_depth=5)\n",
    "\n",
    "# Fit the Decision Tree Regressor to the training data\n",
    "dt_regressor.fit(X_train_reg.values, y_train_reg.values)\n",
    "\n",
    "# Make predictions on the validation data\n",
    "predictions = dt_regressor.predict(X_val_reg.values)\n",
    "\n",
    "# Calculate the mean squared error of the model\n",
    "mse = np.mean((predictions - y_val_reg.values) ** 2)\n",
    "print(f'Validation MSE: {mse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest training and prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate Random Forest Classifier\n",
    "rf_classifier = RandomForest(n_trees=100, max_depth=5)\n",
    "rf_classifier.fit(X_train_clas.values, y_train_clas_numeric)\n",
    "predictions = rf_classifier.predict(X_val_clas.values)\n",
    "accuracy = np.mean(predictions == y_val_clas_numeric)\n",
    "print(f'Validation Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest regressor training and prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate Random Forest Regressor\n",
    "rf_regressor = RandomForestRegressor(n_trees=100, max_depth=5)\n",
    "rf_regressor.fit(X_train_reg.values, y_train_reg.values)\n",
    "predictions = rf_regressor.predict(X_val_reg.values)\n",
    "mse = np.mean((predictions - y_val_reg.values) ** 2)\n",
    "print(f'Validation MSE: {mse}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
