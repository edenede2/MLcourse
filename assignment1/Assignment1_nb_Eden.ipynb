{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification decision tree model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "# Decision Tree Classifier (with Gini impurity)\n",
    "class DecisionTree:\n",
    "    # Initialize the Decision Tree Classifier\n",
    "    def __init__(self, min_samples_split=2, max_depth=5):\n",
    "        # Minimum number of samples required to split an internal node\n",
    "        self.min_samples_split = min_samples_split\n",
    "        # Maximum depth of the tree\n",
    "        self.max_depth = max_depth\n",
    "        # The decision tree\n",
    "        self.tree = None\n",
    "\n",
    "    # Fit the Decision Tree Classifier\n",
    "    def fit(self, X, y):\n",
    "        # Convert string labels to numerical indices\n",
    "        self.classes = np.unique(y)\n",
    "        y = np.array([np.where(self.classes == label)[0][0] for label in y])\n",
    "        # Grow the decision tree\n",
    "        self.tree = self._grow_tree(X, y)\n",
    "    \n",
    "    # Predict the target variable for the input data\n",
    "    def predict(self, X):\n",
    "        predictions = [self._predict(inputs, self.tree) for inputs in X]\n",
    "        # Make predictions for each input based on the decision tree that was grown\n",
    "        return np.array([self.classes[pred] for pred in predictions])\n",
    "\n",
    "    # Calculate the Gini impurity\n",
    "    def _gini(self, y):\n",
    "        # Number of samples\n",
    "        m = len(y)\n",
    "        # Return 1 - sum of the square of the proportion of samples in each class label to the total number of samples in the node\n",
    "        return 1.0 - sum((np.sum(y == c) / m) ** 2 for c in np.unique(y))\n",
    "\n",
    "    # Split the data into left and right subsets based on the feature and threshold\n",
    "    def _split(self, X, y, idx, thresh):\n",
    "        # Get the indices of the samples in the left subset\n",
    "        left_mask = X[:, idx] <= thresh\n",
    "        # Get the indices of the samples in the right subset\n",
    "        right_mask = X[:, idx] > thresh\n",
    "        # Return the left and right subsets of the data\n",
    "        return X[left_mask], X[right_mask], y[left_mask], y[right_mask]\n",
    "\n",
    "\n",
    "    # Find the best split for the data\n",
    "    def _best_split(self, X, y):\n",
    "        # Number of samples (m) and number of features (n)\n",
    "        m, n = X.shape\n",
    "\n",
    "        # If the number of samples is less than or equal to the minimum number of samples required to split, return None\n",
    "        if m <= 1:\n",
    "            return None, None\n",
    "\n",
    "        # Initialize the Gini impurity of the best split to infinity and the best feature and threshold to None\n",
    "        best_gini = 1.0\n",
    "        best_idx, best_thresh = None, None\n",
    "\n",
    "        unique_classes = np.unique(y)\n",
    "        class_count = len(unique_classes)\n",
    "\n",
    "        # For each feature\n",
    "        for idx in range(n):\n",
    "            # Get the thresholds and class labels\n",
    "            thresholds, classes = zip(*sorted(zip(X[:, idx], y)))\n",
    "\n",
    "            # Ensure numeric thresholds\n",
    "            thresholds = np.array(thresholds, dtype=np.float64)\n",
    "\n",
    "            num_left = [0] * class_count\n",
    "            num_right = [np.sum(classes == c) for c in unique_classes]\n",
    "\n",
    "            # For each sample\n",
    "            for i in range(1, m):\n",
    "                class_idx = np.where(unique_classes == classes[i - 1])[0][0]\n",
    "                num_left[class_idx] += 1\n",
    "                num_right[class_idx] -= 1\n",
    "\n",
    "                gini_left = 1.0 - sum((num_left[x] / i) ** 2 for x in range(class_count))\n",
    "                gini_right = 1.0 - sum((num_right[x] / (m - i)) ** 2 for x in range(class_count))\n",
    "                gini = (i * gini_left + (m - i) * gini_right) / m\n",
    "\n",
    "                if thresholds[i] == thresholds[i - 1]:\n",
    "                    continue\n",
    "                if gini < best_gini:\n",
    "                    best_gini = gini\n",
    "                    best_idx = idx\n",
    "                    best_thresh = (thresholds[i] + thresholds[i - 1]) / 2\n",
    "        \n",
    "        return best_idx, best_thresh\n",
    "\n",
    "    # Grow the decision tree\n",
    "    def _grow_tree(self, X, y, depth=0):\n",
    "        \n",
    "        # Get the number of samples for each class label\n",
    "        num_samples_per_class = [np.sum(y == i) for i in np.unique(y)]\n",
    "        # Get the class that occurs\n",
    "        predicted_class = np.argmax(num_samples_per_class)\n",
    "        # Create a node for the decision tree\n",
    "        node = {'predicted_class': predicted_class}\n",
    "\n",
    "        \n",
    "\n",
    "        # If the depth of the tree is less than the maximum depth\n",
    "        if depth < self.max_depth:\n",
    "            # Get the best split\n",
    "            idx, thresh = self._best_split(X, y)\n",
    "            # If the best split is not None\n",
    "            if idx is not None:\n",
    "                # Get the left and right subsets of the data\n",
    "                indices_left = X[:, idx] <= thresh\n",
    "                X_left, y_left = X[indices_left], y[indices_left]\n",
    "                X_right, y_right = X[~indices_left], y[~indices_left]\n",
    "                # Grow the left and right subtrees\n",
    "                node['feature_index'] = idx\n",
    "                node['threshold'] = thresh\n",
    "                node['left'] = self._grow_tree(X_left, y_left, depth + 1)\n",
    "                node['right'] = self._grow_tree(X_right, y_right, depth + 1)\n",
    "        return node\n",
    "\n",
    "    # Predict the target variable for the input data\n",
    "    def _predict(self, inputs, tree):\n",
    "        # If the tree is not a leaf node\n",
    "        if 'threshold' in tree:\n",
    "            # Get the feature index and threshold\n",
    "            feature_index = tree['feature_index']\n",
    "            # Traverse the left or right subtree\n",
    "            if inputs[feature_index] <= tree['threshold']:\n",
    "                return self._predict(inputs, tree['left'])\n",
    "            else:\n",
    "                return self._predict(inputs, tree['right'])\n",
    "        # If the tree is a leaf node\n",
    "        else:\n",
    "            # Return the predicted class\n",
    "            return tree['predicted_class']\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressor decision tree model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# from Models.ClassifierDT import DecisionTree\n",
    "\n",
    "# Decision Tree Regressor (with SSR)\n",
    "class DecisionTreeRegressor(DecisionTree):\n",
    "    # Initialize the Decision Tree Regressor that inherits from the Decision Tree Classifier\n",
    "    def _ssr(self, y):\n",
    "        if len(y) == 0:\n",
    "            return 0\n",
    "        mean_y = np.mean(y)\n",
    "        # Calculate the sum of the squared residuals and return it\n",
    "        return np.sum((y - mean_y) ** 2)\n",
    "\n",
    "    # Find the best split for the data based on the sum of the squared residuals\n",
    "    def _best_split(self, X, y):\n",
    "\n",
    "        # Number of samples (m) and number of features (n)\n",
    "        m, n = X.shape\n",
    "\n",
    "        # If the number of samples is less than or equal to the minimum number of samples required to split, return None\n",
    "        if m <= 1:\n",
    "            return None, None\n",
    "\n",
    "        # Initialize the sum of the squared residuals of the best split to infinity and the best feature and threshold to None\n",
    "        best_ssr = np.inf\n",
    "        best_idx, best_thresh = None, None\n",
    "\n",
    "        # For each feature\n",
    "        for idx in range(n):\n",
    "            # Get the thresholds and values\n",
    "            thresholds, values = zip(*sorted(zip(X[:, idx], y)))\n",
    "            # For each sample\n",
    "            for i in range(1, m):\n",
    "                # Get the left and right subsets of the target variable\n",
    "                y_left, y_right = values[:i], values[i:]\n",
    "                ssr_left, ssr_right = self._ssr(y_left), self._ssr(y_right)\n",
    "\n",
    "                # Calculate the sum of the squared residuals of the left and right subsets\n",
    "                ssr = ssr_left + ssr_right\n",
    "\n",
    "                # Update the best split if the current split has a lower sum of the squared residuals\n",
    "                if thresholds[i] == thresholds[i - 1]:\n",
    "                    continue\n",
    "                if ssr < best_ssr:\n",
    "                    best_ssr = ssr\n",
    "                    best_idx = idx\n",
    "                    best_thresh = (thresholds[i] + thresholds[i - 1]) / 2\n",
    "\n",
    "        # Return the best feature and threshold\n",
    "        return best_idx, best_thresh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# from Models.ClassifierDT import DecisionTree\n",
    "\n",
    "\n",
    "class RandomForest:\n",
    "    def __init__(self, n_trees=100, max_depth=5, min_samples_split=2, n_features=None):\n",
    "        self.n_trees = n_trees\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.n_features = n_features\n",
    "        self.trees = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.trees = []\n",
    "        n_samples, n_features = X.shape\n",
    "        self.n_features = self.n_features or n_features\n",
    "        for _ in range(self.n_trees):\n",
    "            idxs = np.random.choice(n_samples, n_samples, replace=True)\n",
    "            tree = DecisionTree(max_depth=self.max_depth, min_samples_split=self.min_samples_split)\n",
    "            tree.fit(X[idxs], y[idxs])\n",
    "            self.trees.append(tree)\n",
    "\n",
    "    def predict(self, X):\n",
    "        tree_preds = np.array([tree.predict(X) for tree in self.trees])\n",
    "        return np.mean(tree_preds, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest imputation for missing values in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def random_forest_impute(data, categorical_columns, n_trees=100, max_depth=5, min_samples_split=2, n_features=None, max_iter=10, tol=1e-3):\n",
    "    data_imputed = data.copy()\n",
    "    missing_mask = data.isnull()\n",
    "    n_features = n_features or data.shape[1]\n",
    "    \n",
    "    # Temporarily fill NaNs with a placeholder value\n",
    "    placeholder = \"missing\"\n",
    "    data_imputed[categorical_columns] = data_imputed[categorical_columns].fillna(placeholder)\n",
    "    \n",
    "    # One-hot encode categorical variables, without creating separate column for NaN\n",
    "    data_imputed = pd.get_dummies(data_imputed, columns=categorical_columns)\n",
    "    \n",
    "    for iteration in range(max_iter):\n",
    "        prev_data = data_imputed.copy()\n",
    "        for column in categorical_columns:\n",
    "            # Identify the one-hot encoded columns for the current categorical column\n",
    "            cat_columns = [col for col in data_imputed.columns if col.startswith(column + '_')]\n",
    "            \n",
    "            for cat_column in cat_columns:\n",
    "                # Extract the original column name (without the one-hot suffix)\n",
    "                original_column = cat_column.split('_')[0]\n",
    "                \n",
    "                # Handle the placeholder column separately\n",
    "                if cat_column.endswith('_' + placeholder):\n",
    "                    cat_column_name = original_column + '_missing'\n",
    "                    missing_idx = data_imputed[cat_column_name]\n",
    "                else:\n",
    "                    cat_column_name = cat_column\n",
    "                    missing_idx = data_imputed[cat_column_name].isnull()\n",
    "                \n",
    "                if missing_idx.any():\n",
    "                    # Prepare training data\n",
    "                    X_train = data_imputed[~missing_idx].drop(columns=cat_columns)\n",
    "                    y_train = data_imputed[~missing_idx][cat_column_name]\n",
    "                    \n",
    "                    # Prepare data for imputation\n",
    "                    X_impute = data_imputed[missing_idx].drop(columns=cat_columns)\n",
    "                    \n",
    "                    # Train Random Forest\n",
    "                    rf = RandomForest(n_trees=n_trees, max_depth=max_depth, min_samples_split=min_samples_split, n_features=n_features)\n",
    "                    rf.fit(X_train.values, y_train.values)\n",
    "                    \n",
    "                    # Predict missing values\n",
    "                    data_imputed.loc[missing_idx, cat_column_name] = rf.predict(X_impute.values)\n",
    "        \n",
    "        # Check for convergence (i.e., no change in imputed values)\n",
    "        if np.linalg.norm(data_imputed.values - prev_data.values) < tol:\n",
    "            break\n",
    "\n",
    "    return data_imputed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest regressor model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# from Models.ClassifierDT import DecisionTree\n",
    "# from Models.RandomForest import RandomForest\n",
    "\n",
    "# Random Forest Regressor (with averaging)\n",
    "class RandomForestRegressor(RandomForest):\n",
    "    def predict(self, X):\n",
    "        tree_preds = np.array([tree.predict(X) for tree in self.trees])\n",
    "        return np.mean(tree_preds, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries and modules\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# from Models.ClassifierDT import DecisionTree\n",
    "# from Models.RegressionDT import DecisionTreeRegressor\n",
    "# from Models.RandomForest import RandomForest\n",
    "# from Models.RandomForestReg import RandomForestRegressor\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "######### Load the dataset\n",
    " \n",
    "# Load the dataset\n",
    "data = pd.read_csv('/Users/edeneldar/Library/Mobile Documents/com~apple~CloudDocs/ML learn.worktrees/origin/mainEden/assignment1/assignment-1-data.csv')\n",
    "\n",
    "# Rename the columns for easier access\n",
    "data = data[['Brand', 'Screen_Size', 'RAM', 'Processor', 'GPU', 'GPU_Type', 'Resolution', 'Condition', 'Price']]\n",
    "data.columns = ['Brand', 'Screen-Size', 'RAM', 'Processor', 'GPU', 'GPU-Type', 'Resolution', 'Condition', 'Price']\n",
    "\n",
    "\n",
    "# # handle midding values\n",
    "# processors_catagory = data['Processor'].unique()\n",
    "# len(processors_catagory)\n",
    "\n",
    "# #  create histrograms of the data to see how each catagory is distributed for the GPU, GPU_Type, and Resolution columns under each processor type\n",
    "# for processor in processors_catagory:\n",
    "#     fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
    "#     for i, column in enumerate(['GPU', 'GPU_Type', 'Resolution']):\n",
    "#         sns.histplot(data[data['Processor'] == processor][column], ax=ax[i])\n",
    "#         ax[i].set_title(f'{column} for {processor}')\n",
    "#     plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing values imputation with random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the categorical columns\n",
    "categorical_columns = ['Brand', 'Processor', 'GPU', 'GPU-Type', 'Resolution', 'Condition']\n",
    "\n",
    "\n",
    "# Impute missing values using Random Forest\n",
    "imputed_data = random_forest_impute(data, categorical_columns, n_trees=100, max_depth=5, min_samples_split=2, n_features=None, max_iter=10, tol=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(221,\n",
       " b'2.0.0 closing connection ffacd0b85a97d-35ef5fc0f29sm1250816f8f.105 - gsmtp')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imputed_data.to_csv('/tmp/imputed_data.csv', index=False)\n",
    "\n",
    "import smtplib\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from email.mime.text import MIMEText\n",
    "from email.mime.base import MIMEBase\n",
    "\n",
    "from email import encoders\n",
    "\n",
    "# send the csv file to the email\n",
    "fromaddr = \"edenstream988@gmail.com\"\n",
    "password = \"xvwd qwqs ngev sbmd\"\n",
    "toaddr = \"edenede2@gmail.com\"\n",
    "\n",
    "msg = MIMEMultipart()\n",
    "\n",
    "msg['From'] = fromaddr\n",
    "msg['To'] = toaddr\n",
    "msg['Subject'] = \"Imputed data\"\n",
    "\n",
    "body = \"Imputed data\"\n",
    "\n",
    "msg.attach(MIMEText(body, 'plain'))\n",
    "\n",
    "filename = \"imputed_data.csv\"\n",
    "attachment = open(\"/tmp/imputed_data.csv\", \"rb\")\n",
    "\n",
    "p = MIMEBase('application', 'octet-stream')\n",
    "p.set_payload((attachment).read())\n",
    "encoders.encode_base64(p)\n",
    "p.add_header('Content-Disposition', \"attachment; filename= %s\" % filename)\n",
    "\n",
    "msg.attach(p)\n",
    "\n",
    "s = smtplib.SMTP('smtp.gmail.com', 587)\n",
    "s.starttls()\n",
    "s.login(fromaddr, password)\n",
    "text = msg.as_string()\n",
    "s.sendmail(fromaddr, toaddr, text)\n",
    "s.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Correct the values of the 'Condition' column to 'New' and 'Refurbished'\n",
    "data['Condition'] = data['Condition'].apply(lambda x: 'New' if x == 'New' or x == 'Open box' else 'Refurbished')\n",
    "\n",
    "# Convert categorical features to numerical values using one-hot encoding\n",
    "data = pd.get_dummies(data, columns=['Brand', 'Processor', 'GPU', 'GPU_Type', 'Resolution'])\n",
    "\n",
    "reg_data = pd.get_dummies(data, columns=['Condition'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the data into training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### Split the data\n",
    "\n",
    "# Split the data\n",
    "train_data = data.iloc[0:2058]\n",
    "val_data = data.iloc[2058:2499]\n",
    "test_data = data.iloc[2499:2939]\n",
    "\n",
    "# Split the data for regression\n",
    "train_data_reg = reg_data.iloc[0:2058]\n",
    "val_data_reg = reg_data.iloc[2058:2499]\n",
    "test_data_reg = reg_data.iloc[2499:2939]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract features and target variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features and target variables\n",
    "X_train_clas = train_data.drop(columns=['Condition'])\n",
    "X_train_reg = train_data_reg.drop(columns=['Price'])\n",
    "y_train_clas = train_data['Condition']\n",
    "y_train_reg = train_data_reg['Price']\n",
    "\n",
    "X_val_clas = val_data.drop(columns=['Condition'])\n",
    "X_val_reg = val_data_reg.drop(columns=['Price'])\n",
    "y_val_clas = val_data['Condition']\n",
    "y_val_reg = val_data_reg['Price']\n",
    "\n",
    "X_test_clas = test_data.drop(columns=['Condition'])\n",
    "X_test_reg = test_data_reg.drop(columns=['Price'])\n",
    "y_test_clas = test_data['Condition']\n",
    "y_test_reg = test_data_reg['Price']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert string labels to numerical indices for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert string labels to numerical indices\n",
    "class_map = {label: idx for idx, label in enumerate(np.unique(y_train_clas))}\n",
    "y_train_clas_numeric = np.array([class_map[label] for label in y_train_clas])\n",
    "y_val_clas_numeric = np.array([class_map[label] for label in y_val_clas])\n",
    "y_test_clas_numeric = np.array([class_map[label] for label in y_test_clas])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision tree training and prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7959183673469388\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate Decision Tree Classifier with max_depth=5\n",
    "dt_classifier = DecisionTree(max_depth=5)\n",
    "dt_classifier.fit(X_train_clas.values, y_train_clas_numeric)\n",
    "predictions = dt_classifier.predict(X_val_clas.values)\n",
    "accuracy = np.mean(predictions == y_val_clas_numeric)\n",
    "print(f'Validation Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision tree regressor training and prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation MSE: 177610.21589024944\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate Decision Tree Regressor with max_depth=5\n",
    "dt_regressor = DecisionTreeRegressor(max_depth=5)\n",
    "\n",
    "# Fit the Decision Tree Regressor to the training data\n",
    "dt_regressor.fit(X_train_reg.values, y_train_reg.values)\n",
    "\n",
    "# Make predictions on the validation data\n",
    "predictions = dt_regressor.predict(X_val_reg.values)\n",
    "\n",
    "# Calculate the mean squared error of the model\n",
    "mse = np.mean((predictions - y_val_reg.values) ** 2)\n",
    "print(f'Validation MSE: {mse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest training and prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate Random Forest Classifier\n",
    "rf_classifier = RandomForest(n_trees=100, max_depth=5)\n",
    "rf_classifier.fit(X_train_clas.values, y_train_clas_numeric)\n",
    "predictions = rf_classifier.predict(X_val_clas.values)\n",
    "accuracy = np.mean(predictions == y_val_clas_numeric)\n",
    "print(f'Validation Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest regressor training and prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate Random Forest Regressor\n",
    "rf_regressor = RandomForestRegressor(n_trees=100, max_depth=5)\n",
    "rf_regressor.fit(X_train_reg.values, y_train_reg.values)\n",
    "predictions = rf_regressor.predict(X_val_reg.values)\n",
    "mse = np.mean((predictions - y_val_reg.values) ** 2)\n",
    "print(f'Validation MSE: {mse}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
